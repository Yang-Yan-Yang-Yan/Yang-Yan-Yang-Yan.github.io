<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into nat"><meta property="og:type" content="blog"><meta property="og:title" content="My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"><meta property="og:url" content="http://example.com/2023/03/07/note-3/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into nat"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/images/note-3_images/3_model_1.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_model_2.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_outcome_1.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_outcome_2.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_outcome_5.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_outcome_3.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_outcome_3.png"><meta property="og:image" content="http://example.com/images/note-3_images/3_outcome_6.png"><meta property="article:published_time" content="2023-03-06T16:00:00.000Z"><meta property="article:modified_time" content="2023-03-06T16:26:38.644Z"><meta property="article:author" content="John Doe"><meta property="article:tag" content="ACL 2021"><meta property="article:tag" content="NLG"><meta property="article:tag" content="Subword"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/images/note-3_images/3_model_1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2023/03/07/note-3/"},"headline":"My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation","image":["http://example.com/images/note-3_images/3_model_1.png","http://example.com/images/note-3_images/3_model_2.png","http://example.com/images/note-3_images/3_outcome_1.png","http://example.com/images/note-3_images/3_outcome_2.png","http://example.com/images/note-3_images/3_outcome_5.png","http://example.com/images/note-3_images/3_outcome_3.png","http://example.com/images/note-3_images/3_outcome_3.png","http://example.com/images/note-3_images/3_outcome_6.png"],"datePublished":"2023-03-06T16:00:00.000Z","dateModified":"2023-03-06T16:26:38.644Z","author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into nat"}</script><link rel="canonical" href="http://example.com/2023/03/07/note-3/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-06T16:00:00.000Z" title="2023/3/7 00:00:00">2023-03-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-06T16:26:38.644Z" title="2023/3/7 00:26:38">2023-03-07</time></span><span class="level-item"><a class="link-muted" href="/categories/My-Note/">My Note</a></span><span class="level-item">15 minutes read (About 2286 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</h1><div class="content"><h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a><strong>背景</strong>：</h3><p>基于 <strong>pretrain-finetune</strong> 的模型在 <strong>pretrain</strong> 时使用<strong>统一的语料库 (one-size-fits-all vocabulary)</strong>，而 <strong>finetune</strong> 时的<strong>语料库则随具体任务而不同</strong>。<br>pretrain 时的语料库和 finetune 时的语料库中的 <strong>subwords 分布往往会有所不同</strong>，这导致了: </p>
<ul>
<li><strong>pretrain</strong> 时学到的 <strong>subwords 划分会更加细粒度</strong>从而覆盖其更大的语料库，但这会使得 finetune 时的<strong> exposure bias 更严重</strong>且<strong>计算开销更大</strong></li>
<li><strong>在 pretrain 时不常见，但在 finetune 时常见的 token</strong> 可能会被<strong>错误划分为 subwords</strong>，导致语义保留不佳</li>
</ul>
<hr>
<p><br></p>
<h3 id="本文："><a href="#本文：" class="headerlink" title="本文："></a><strong>本文</strong>：</h3><ul>
<li><strong>目标</strong>：改善上下游任务中由于 <strong>subwords 分布不同</strong>导致的下游任务中部分token表示不佳 <strong>(under-represented)</strong></li>
<li><strong>方法</strong>：单独训练一个 <strong>embedding generator</strong>，输入一个 token，<strong>根据其 subwords 和 hyperwords 的词向量来得到该token的词向量</strong>，改善下游任务中 <strong>under-represented token</strong> 的词向量表达</li>
<li><strong>模型</strong>：<strong>AVG-EG</strong>; <strong>ATT-EG</strong>; <strong>PATT-EG</strong></li>
<li><strong>创新点</strong>：(1) 从克服 <strong>subwords 分布差异</strong> 的角度出发，有效改善了模型; (2) <strong>计算效率高</strong>，在简单地<strong>单独训练</strong>后可以<strong>即插即用 (plug-and-play)</strong> ; (3) 对 <strong>under-represented token</strong> 的词向量表示进行了多个方法的探索，包括<strong>平均</strong>，基于<strong>注意力机制</strong>和基于<strong>语素 (morphemes) 信息</strong>的方法。</li>
</ul>
<hr>
<p><br></p>
<h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a><strong>方法</strong>：</h3><h5 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a><strong>模型架构</strong>：</h5><p>模型基于标准的 pretrain-finetune 模式构建</p>
<ul>
<li>在 <strong>pretrain</strong> 时，<strong>额外训练一个 generator</strong> ，其可以<strong>输出任何输入 token 的词向量</strong></li>
<li>在 <strong>finetune</strong> 时，分为 embedding layer 和 inner layer 两部分，<strong>embedding layer</strong> 中首先基于下游任务的语料库学习 subwords 划分方法以及对应的词表，其中的 <strong>token 如果存在于预训练模型的词表中，则直接使用预训练的 embedding layer 得到词向量</strong>，<strong>否则使用 generator 生成其词向量</strong>；inner layer 部分延用预训练模型的参数。<br><img src="/images/note-3_images/3_model_1.png" alt></li>
</ul>
<h5 id="构建-Embedding-Generator："><a href="#构建-Embedding-Generator：" class="headerlink" title="构建 Embedding Generator："></a><strong>构建 Embedding Generator</strong>：</h5><p><strong>Subwords &amp; Hyperwords:</strong> </p>
<ul>
<li>一个 token 的<strong>语义信息</strong>与其 <strong>subwords</strong> 和 <strong>hyperwords</strong> 有关，以 motorcycle 为例，其 subwords 包含 motor 和 cycle，反之，motor 的 hyperwords 包含 motorcycle</li>
<li>在<strong>下游任务词表</strong>中发现一个<strong>上游任务词表中不存在的token (unseen token)</strong> 时，<strong>获取</strong>上游词表中其所有的 <strong>subwords</strong> 和 <strong>hyperwords</strong> ，根据这些 token 的 embedding 计算 unseen token 的 embedding<br><br></li>
</ul>
<p><strong>AVG-EG: Averaging-Based Embedding Generator</strong></p>
<ul>
<li>将 unseen token 的所有 subwords 和 hyperwords 的 <strong>embedding 平均</strong>作为 unseen token 的 embedding<script type="math/tex; mode=display">
G(w)=\frac{1}{\left|S_{m}(w)\right|} \sum_{w^{\prime} \in S_{m}(w)} E\left(w^{\prime}\right)</script></li>
</ul>
<p>其中：</p>
<ul>
<li>$w$ 为下游词表的 unseen token</li>
<li>$w^{\prime}$ 为上游词表中与 $w$ 有关的 token (subwords or hyperwords)</li>
<li>$S_{m}(w)$ 是 $w^{\prime}$ 的集合</li>
<li>$E\left(w^{\prime}\right)$ 即为 $w^{\prime}$ 在预训练模型的 embedding 层得到的词向量<br><br></li>
</ul>
<p><strong>ATT-EG: Attention-Based Embedding Generator</strong></p>
<ul>
<li>将 $S_{m}(w)$ 中的所有 $w^{\prime}$ 通过 attention 机制计算  $w$ 的词向量<script type="math/tex; mode=display">
G(w)=\frac{1}{\left|S_{m}(w)\right|} \sum_{w^{\prime} \in S_{m}(w)} \alpha\left(w^{\prime}\right) \cdot E\left(w^{\prime}\right)</script><script type="math/tex; mode=display">
\alpha\left(w^{\prime}\right)=\frac{\exp \left(\mathbf{W}^{\top} E\left(w^{\prime}\right)\right)}{\sum_{w^{\prime \prime} \in S_{m}(w)} \exp \left(\mathbf{W}^{\top} E\left(w^{\prime \prime}\right)\right)}</script>其中：</li>
<li>$\mathbf{W}$ 为可学习的参数<br><br></li>
</ul>
<p><strong>PATT-EG: Position-Aware Attention-Based Embedding Generator</strong></p>
<ul>
<li>为 $w$ 和 $w^{\prime}$ 定义了<strong>六种关系</strong></li>
<li>如果 $w$ 是 $w^{\prime}$ 的 <strong>subword</strong>，那 $w$ 可能是 $w^{\prime}$ 的 <strong>prefix / infix / suffix</strong></li>
<li>如果 $w$ 是 $w^{\prime}$ 的 <strong>hyperword</strong>，那 $w^{\prime}$ 可能是 $w$ 的 prefix / infix / suffix</li>
<li>根据 $w$ 和 $w^{\prime}$ 之间的关系确定计算公式<script type="math/tex; mode=display">
G(w)=\frac{1}{\left|S_{m}(w)\right|} \sum_{w^{\prime} \in S_{m}(w)} \alpha\left(w^{\prime}\right) \cdot E\left(w^{\prime}\right)</script><script type="math/tex; mode=display">
\alpha\left(w^{\prime}\right)=\frac{\exp \left(\mathbf{I{W_r}}E\left(w^{\prime}\right)\right)}{\sum_{w^{\prime \prime} \in S_{m}(w)} \exp \left(\mathbf{I{W_r}} E\left(w^{\prime \prime}\right)\right)}</script>其中：</li>
<li>$\mathbf{W_r}$ 为可学习的参数</li>
<li>$\mathbf{I}$ 为 one-hot 向量，指示 $w$ 和 $w^{\prime}$ 之间的关系<br><br></li>
</ul>
<h5 id="训练-Embedding-Generator："><a href="#训练-Embedding-Generator：" class="headerlink" title="训练 Embedding Generator："></a><strong>训练 Embedding Generator：</strong></h5><p><img src="/images/note-3_images/3_model_2.png" alt><br><br></p>
<p><strong>预处理上游语料库获取 unseen token</strong></p>
<ul>
<li>在上游语料库中<strong>随机选取连续的 token</strong> ，<strong>拼接</strong>为一个 unseen token</li>
<li>在上游语料库中<strong>随机选取 token</strong> ，将其<strong>拆分</strong>为多个 unseen token<br><br></li>
</ul>
<p><strong>Reusing Pre-training Loss</strong></p>
<ul>
<li>根据上面获取的 unseen token，使用 embedding generator 获取其 embedding，<strong>代入预训练模型的 inner layer，进行预训练的任务</strong></li>
<li><strong>inner layer 的参数保持不变</strong>，<strong>预训练任务的loss也保持不变</strong>，从而尽可能使 embedding generator 生成的词向量与原本 embedding layer 生成的词向量在<strong>同一个空间之中</strong></li>
<li>这部分的 <strong>loss</strong> 记作 $\mathcal{L}_p\left(s^{\prime}\right)$<br><br></li>
</ul>
<p><strong>Knowledge Distillation</strong></p>
<ul>
<li>计算 <strong>embedding generator 生成的词向量</strong>与<strong>原本 embedding layer 生成的词向量</strong>的<strong>距离</strong>，使其<strong>尽可能小</strong></li>
<li>进一步帮助 embedding generator 生成的词向量与原本 embedding layer 生成的词向量<strong>在同一个空间之中</strong><script type="math/tex; mode=display">
\mathcal{L}_d\left(s^p, s^{\prime}\right)=\frac{1}{|s|} \sum_{w \in s}\left\|h^p(w)-h^{\prime}(w)\right\|^2</script><br></li>
</ul>
<p><strong>训练embedding generator的总loss</strong></p>
<script type="math/tex; mode=display">
\mathcal{L}\left(s^p, s^{\prime}\right)=\mathcal{L}_p\left(s^{\prime}\right)+\lambda \mathcal{L}_d\left(s^p, s^{\prime}\right)</script><p>其中：</p>
<ul>
<li>$\lambda$ 是权衡两个loss占比的系数</li>
</ul>
<hr>
<p><br></p>
<h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a><strong>实验</strong>：</h3><h5 id="Domain-Adaptation"><a href="#Domain-Adaptation" class="headerlink" title="Domain Adaptation:"></a><strong>Domain Adaptation:</strong></h5><p><strong>Dataset:</strong></p>
<ul>
<li><strong>预训练</strong>语料 (<strong>out-of-domain</strong>): <strong>LDC</strong></li>
<li><strong>下游任务</strong>语料 (<strong>in-domain</strong>): 收集于<strong>UM-Corpus</strong> (domian: <strong>Thesis &amp; Laws</strong>)</li>
<li>根据以上情境可知，<strong>unseen token</strong>存在于<strong>in-domain</strong>语料中，不存在于out-of-domain语料中<br><br></li>
</ul>
<p><strong>Metrtics:</strong> </p>
<ul>
<li><strong>cased BLEU</strong>: caculated by mteval-v13a.pl.<br><br></li>
</ul>
<p><strong>Baselines:</strong><br>以下所有模型都是基于 <strong>Transformer</strong> 构建的。</p>
<ul>
<li><strong>Single-Run:</strong> 仅使用 in-domain 语料进行训练</li>
<li><strong>Pretrain-Finetune:</strong> 在 out-of-domain 语料上预训练，再在 in-domain 语料上进行finetune</li>
</ul>
<p>基于 <strong>Task-Specific Vocabulary</strong> 改进: 对预训练模型各个层进行重新训练</p>
<ul>
<li><strong>Downstream Vocab:</strong> 使用 in-domain 语料重新训练预训练模型</li>
<li><strong>Joint Vocab:</strong> 从 in &amp; out of domain 的多大语料库提取语料重新训练预训练模型</li>
<li><strong>BPE-Drop:</strong> 对预训练语料进行 subword regularization 来提高其鲁棒性</li>
</ul>
<p>基于 <strong>Embedding Generator</strong> 改进: 保留原本预训练模型，为 unseen token 训练 generator 来获得其 embedding</p>
<ul>
<li><strong>Random Init:</strong> 随机初始化 unseen token 的 embedding</li>
<li><strong>Word2Vec:</strong> 在 in-domain 语料上训练 Word2Vec 模型获取 unseen token 的 embedding</li>
<li><strong>Embedding Recon:</strong> 根据 unseen token 的 subwords 来获取其 embedding</li>
</ul>
<p>在下游任务上<strong>重新训练 Embedding 层</strong>: 仅重新训练预训练模型的 embedding 层</p>
<ul>
<li><strong>Independent Encoder:</strong> 随机初始化 embedding 层</li>
<li><strong>CBOW:</strong> 在 in-domain 语料上用 CBOW 训练得到 embedding 层<br><br></li>
</ul>
<p><strong>实验结果:</strong><br><img src="/images/note-3_images/3_outcome_1.png" alt></p>
<ul>
<li><strong>finetune</strong> 一个预训练模型的效果<strong>要差于</strong>直接<strong>在 in-domain 语料上训练</strong>的模型</li>
<li>应用 <strong>Task-Specific Vocabulary 能提升效果</strong></li>
<li><strong>Random Init</strong> 效果要<strong>优于</strong> <strong>Word2Vec</strong> 和 <strong>Embedding Recon</strong> ，这可能是是因为<strong>后两者</strong>完全基于 in-domain 语料进行训练，<strong>得到的词向量所在空间与预训练模型词向量所在空间差异过大</strong></li>
<li>本文提出的模型效果相对最好，可见使用subwords和hyperwords确实可以辅助embedding的获取</li>
<li>加入了 <strong>Knowledge Distillation</strong> 的模型效果进一步提升了，因为这进一步缩小了使用embedding layer和embedding generator产生的词向量的差异<br><br></li>
</ul>
<h5 id="Knowledge-Transferring："><a href="#Knowledge-Transferring：" class="headerlink" title="Knowledge Transferring："></a><strong>Knowledge Transferring：</strong></h5><p>分别进行了 <strong>Machine Translation</strong> 和 <strong>Question Generation</strong> 两个任务<br><strong>Dataset:</strong></p>
<ul>
<li><strong>Machine Translation</strong>: <strong>WMT14</strong></li>
<li><strong>Question Generation</strong>: <strong>SQuAD v1.1</strong> (input: answer &amp; passage; output: question)<br><br></li>
</ul>
<p><strong>Metrtics:</strong> </p>
<ul>
<li><strong>Machine Translation</strong>: <strong>BLEU</strong></li>
<li><strong>Question Generation</strong>: <strong>ROUGE-L</strong>; <strong>BLEU</strong>; <strong>METEOR</strong><br><br></li>
</ul>
<p><strong>Baselines:</strong></p>
<ul>
<li><strong>M-BERT</strong></li>
<li><strong>M-BART</strong></li>
<li><strong>Random Init</strong>: 与M-BERT模型架构相同，但是随机初始化的，即没有预训练权重<br><br></li>
</ul>
<p><strong>实验结果:</strong><br><img src="/images/note-3_images/3_outcome_2.png" alt></p>
<ul>
<li>本文提出的模型<strong>效果较好</strong>，且同时<strong>减少了参数量</strong>，<strong>提高了速度</strong>，这是因为 <strong>embedding generator 减少了词表长度</strong></li>
</ul>
<hr>
<p><br></p>
<h3 id="分析："><a href="#分析：" class="headerlink" title="分析："></a><strong>分析</strong>：</h3><h5 id="Impact-of-Subword-Granularity："><a href="#Impact-of-Subword-Granularity：" class="headerlink" title="Impact of Subword Granularity："></a><strong>Impact of Subword Granularity：</strong></h5><p><img src="/images/note-3_images/3_outcome_5.png" alt></p>
<ul>
<li>不同的<strong>分词粒度 (token granularity)</strong> 会影响模型的<strong>推理速度 (inference speed)</strong> 以及<strong>各token的暴露偏差 (exposure bias)</strong> ，上图中 exposure bias 用 <strong>Inference ECE (Expected Calibration Error)</strong> 表示</li>
<li>如图可见，适当的分词方式可以提高推理速度，也可以减轻 exposure bias<br><br></li>
</ul>
<h5 id="Impact-of-Embedding-Transfer："><a href="#Impact-of-Embedding-Transfer：" class="headerlink" title="Impact of Embedding Transfer："></a><strong>Impact of Embedding Transfer：</strong></h5><p><img src="/images/note-3_images/3_outcome_3.png" alt></p>
<ul>
<li>上图展示了 <strong>M-BERT</strong> 模型使用本文 embedding generator 方法 (+Ours)，以及不使用本文方法 (w/M-BERT) 的模型效果</li>
<li><strong>横坐标为 embedding generator 的训练步数</strong>，由于w/M-BERT没有embedding generator，因此其为一条直线</li>
<li>加入了 <strong>embedding generator</strong> 在<strong>各个训练步数下都优于 w/M-BERT 模型</strong><br><br></li>
</ul>
<h5 id="Computational-Costs："><a href="#Computational-Costs：" class="headerlink" title="Computational Costs："></a><strong>Computational Costs：</strong></h5><p><img src="/images/note-3_images/3_outcome_3.png" alt></p>
<ul>
<li>如图可见，使用本文 embedding generator 方法 (+Ours)的模型<strong>很快收敛</strong>了，说明需要的<strong>计算开销不大</strong></li>
<li>推测<strong>训练速度快</strong>得益于 <strong>subwords 组成复合词的共性</strong>以及<strong>本文模型架构简单</strong></li>
<li>相比于 finetune 的时间，<strong>训练 embedding generator 的时间微不足道</strong>且 <strong>embedding generator 训练完成后可以复用于多个下游任务</strong><br><br></li>
</ul>
<h5 id="Qualitative-Analysis："><a href="#Qualitative-Analysis：" class="headerlink" title="Qualitative Analysis："></a><strong>Qualitative Analysis：</strong></h5><p><img src="/images/note-3_images/3_outcome_6.png" alt></p>
<ul>
<li>上图为一个翻译示例</li>
<li><strong>M-BERT</strong> 模型将 token: dankbar <strong>不合适地划分开</strong>，导致翻译结果不佳</li>
<li>本文模型<strong>正确划分</strong>，<strong>翻译准确</strong></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</p><p><a href="http://example.com/2023/03/07/note-3/">http://example.com/2023/03/07/note-3/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>John Doe</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-03-07</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-03-07</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ACL-2021/">ACL 2021</a><a class="link-muted mr-2" rel="tag" href="/tags/NLG/">NLG</a><a class="link-muted mr-2" rel="tag" href="/tags/Subword/">Subword</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/02/15/note-2/"><span class="level-item">note-2</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/%E5%A4%B4%E5%83%8F.png" alt="Yang Yan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Yang Yan</p><p class="is-size-6 is-block">Master of Software Engineering</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai University of Finance and Economics</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">6</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Yang-Yan-Yang-Yan" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Yang-Yan-Yang-Yan"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/My-Note/"><span class="level-start"><span class="level-item">My Note</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-06T16:00:00.000Z">2023-03-07</time></p><p class="title"><a href="/2023/03/07/note-3/">My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</a></p><p class="categories"><a href="/categories/My-Note/">My Note</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-15T14:07:37.000Z">2023-02-15</time></p><p class="title"><a href="/2023/02/15/note-2/">note-2</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-13T15:54:31.000Z">2023-02-13</time></p><p class="title"><a href="/2023/02/13/note-1/">My-Note-1 | Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</a></p><p class="categories"><a href="/categories/My-Note/">My Note</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-30T12:40:25.903Z">2022-12-30</time></p><p class="title"><a href="/2022/12/30/hello-world/">Hello World</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ACL-2021/"><span class="tag">ACL 2021</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Aligned-Augmentation/"><span class="tag">Aligned Augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Contrastive-Learning/"><span class="tag">Contrastive Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MNMT/"><span class="tag">MNMT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLG/"><span class="tag">NLG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword/"><span class="tag">Subword</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2023 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>