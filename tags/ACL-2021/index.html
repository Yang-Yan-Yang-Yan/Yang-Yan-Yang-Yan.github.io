<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: ACL 2021 - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Hexo","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">ACL 2021</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-06T16:00:00.000Z" title="2023/3/7 00:00:00">2023-03-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-06T16:26:38.644Z" title="2023/3/7 00:26:38">2023-03-07</time></span><span class="level-item"><a class="link-muted" href="/categories/My-Note/">My Note</a></span><span class="level-item">15 minutes read (About 2286 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/07/note-3/">My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</a></h1><div class="content"><h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a><strong>背景</strong>：</h3><p>基于 <strong>pretrain-finetune</strong> 的模型在 <strong>pretrain</strong> 时使用<strong>统一的语料库 (one-size-fits-all vocabulary)</strong>，而 <strong>finetune</strong> 时的<strong>语料库则随具体任务而不同</strong>。<br>pretrain 时的语料库和 finetune 时的语料库中的 <strong>subwords 分布往往会有所不同</strong>，这导致了: </p>
<ul>
<li><strong>pretrain</strong> 时学到的 <strong>subwords 划分会更加细粒度</strong>从而覆盖其更大的语料库，但这会使得 finetune 时的<strong> exposure bias 更严重</strong>且<strong>计算开销更大</strong></li>
<li><strong>在 pretrain 时不常见，但在 finetune 时常见的 token</strong> 可能会被<strong>错误划分为 subwords</strong>，导致语义保留不佳</li>
</ul>
<hr>
<p><br></p>
<h3 id="本文："><a href="#本文：" class="headerlink" title="本文："></a><strong>本文</strong>：</h3><ul>
<li><strong>目标</strong>：改善上下游任务中由于 <strong>subwords 分布不同</strong>导致的下游任务中部分token表示不佳 <strong>(under-represented)</strong></li>
<li><strong>方法</strong>：单独训练一个 <strong>embedding generator</strong>，输入一个 token，<strong>根据其 subwords 和 hyperwords 的词向量来得到该token的词向量</strong>，改善下游任务中 <strong>under-represented token</strong> 的词向量表达</li>
<li><strong>模型</strong>：<strong>AVG-EG</strong>; <strong>ATT-EG</strong>; <strong>PATT-EG</strong></li>
<li><strong>创新点</strong>：(1) 从克服 <strong>subwords 分布差异</strong> 的角度出发，有效改善了模型; (2) <strong>计算效率高</strong>，在简单地<strong>单独训练</strong>后可以<strong>即插即用 (plug-and-play)</strong> ; (3) 对 <strong>under-represented token</strong> 的词向量表示进行了多个方法的探索，包括<strong>平均</strong>，基于<strong>注意力机制</strong>和基于<strong>语素 (morphemes) 信息</strong>的方法。</li>
</ul>
<hr>
<p><br></p>
<h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a><strong>方法</strong>：</h3><h5 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a><strong>模型架构</strong>：</h5><p>模型基于标准的 pretrain-finetune 模式构建</p>
<ul>
<li>在 <strong>pretrain</strong> 时，<strong>额外训练一个 generator</strong> ，其可以<strong>输出任何输入 token 的词向量</strong></li>
<li>在 <strong>finetune</strong> 时，分为 embedding layer 和 inner layer 两部分，<strong>embedding layer</strong> 中首先基于下游任务的语料库学习 subwords 划分方法以及对应的词表，其中的 <strong>token 如果存在于预训练模型的词表中，则直接使用预训练的 embedding layer 得到词向量</strong>，<strong>否则使用 generator 生成其词向量</strong>；inner layer 部分延用预训练模型的参数。<br><img src="/images/note-3_images/3_model_1.png" alt></li>
</ul>
<h5 id="构建-Embedding-Generator："><a href="#构建-Embedding-Generator：" class="headerlink" title="构建 Embedding Generator："></a><strong>构建 Embedding Generator</strong>：</h5><p><strong>Subwords &amp; Hyperwords:</strong> </p>
<ul>
<li>一个 token 的<strong>语义信息</strong>与其 <strong>subwords</strong> 和 <strong>hyperwords</strong> 有关，以 motorcycle 为例，其 subwords 包含 motor 和 cycle，反之，motor 的 hyperwords 包含 motorcycle</li>
<li>在<strong>下游任务词表</strong>中发现一个<strong>上游任务词表中不存在的token (unseen token)</strong> 时，<strong>获取</strong>上游词表中其所有的 <strong>subwords</strong> 和 <strong>hyperwords</strong> ，根据这些 token 的 embedding 计算 unseen token 的 embedding<br><br></li>
</ul>
<p><strong>AVG-EG: Averaging-Based Embedding Generator</strong></p>
<ul>
<li>将 unseen token 的所有 subwords 和 hyperwords 的 <strong>embedding 平均</strong>作为 unseen token 的 embedding<script type="math/tex; mode=display">
G(w)=\frac{1}{\left|S_{m}(w)\right|} \sum_{w^{\prime} \in S_{m}(w)} E\left(w^{\prime}\right)</script></li>
</ul>
<p>其中：</p>
<ul>
<li>$w$ 为下游词表的 unseen token</li>
<li>$w^{\prime}$ 为上游词表中与 $w$ 有关的 token (subwords or hyperwords)</li>
<li>$S_{m}(w)$ 是 $w^{\prime}$ 的集合</li>
<li>$E\left(w^{\prime}\right)$ 即为 $w^{\prime}$ 在预训练模型的 embedding 层得到的词向量<br><br></li>
</ul>
<p><strong>ATT-EG: Attention-Based Embedding Generator</strong></p>
<ul>
<li>将 $S_{m}(w)$ 中的所有 $w^{\prime}$ 通过 attention 机制计算  $w$ 的词向量<script type="math/tex; mode=display">
G(w)=\frac{1}{\left|S_{m}(w)\right|} \sum_{w^{\prime} \in S_{m}(w)} \alpha\left(w^{\prime}\right) \cdot E\left(w^{\prime}\right)</script><script type="math/tex; mode=display">
\alpha\left(w^{\prime}\right)=\frac{\exp \left(\mathbf{W}^{\top} E\left(w^{\prime}\right)\right)}{\sum_{w^{\prime \prime} \in S_{m}(w)} \exp \left(\mathbf{W}^{\top} E\left(w^{\prime \prime}\right)\right)}</script>其中：</li>
<li>$\mathbf{W}$ 为可学习的参数<br><br></li>
</ul>
<p><strong>PATT-EG: Position-Aware Attention-Based Embedding Generator</strong></p>
<ul>
<li>为 $w$ 和 $w^{\prime}$ 定义了<strong>六种关系</strong></li>
<li>如果 $w$ 是 $w^{\prime}$ 的 <strong>subword</strong>，那 $w$ 可能是 $w^{\prime}$ 的 <strong>prefix / infix / suffix</strong></li>
<li>如果 $w$ 是 $w^{\prime}$ 的 <strong>hyperword</strong>，那 $w^{\prime}$ 可能是 $w$ 的 prefix / infix / suffix</li>
<li>根据 $w$ 和 $w^{\prime}$ 之间的关系确定计算公式<script type="math/tex; mode=display">
G(w)=\frac{1}{\left|S_{m}(w)\right|} \sum_{w^{\prime} \in S_{m}(w)} \alpha\left(w^{\prime}\right) \cdot E\left(w^{\prime}\right)</script><script type="math/tex; mode=display">
\alpha\left(w^{\prime}\right)=\frac{\exp \left(\mathbf{I{W_r}}E\left(w^{\prime}\right)\right)}{\sum_{w^{\prime \prime} \in S_{m}(w)} \exp \left(\mathbf{I{W_r}} E\left(w^{\prime \prime}\right)\right)}</script>其中：</li>
<li>$\mathbf{W_r}$ 为可学习的参数</li>
<li>$\mathbf{I}$ 为 one-hot 向量，指示 $w$ 和 $w^{\prime}$ 之间的关系<br><br></li>
</ul>
<h5 id="训练-Embedding-Generator："><a href="#训练-Embedding-Generator：" class="headerlink" title="训练 Embedding Generator："></a><strong>训练 Embedding Generator：</strong></h5><p><img src="/images/note-3_images/3_model_2.png" alt><br><br></p>
<p><strong>预处理上游语料库获取 unseen token</strong></p>
<ul>
<li>在上游语料库中<strong>随机选取连续的 token</strong> ，<strong>拼接</strong>为一个 unseen token</li>
<li>在上游语料库中<strong>随机选取 token</strong> ，将其<strong>拆分</strong>为多个 unseen token<br><br></li>
</ul>
<p><strong>Reusing Pre-training Loss</strong></p>
<ul>
<li>根据上面获取的 unseen token，使用 embedding generator 获取其 embedding，<strong>代入预训练模型的 inner layer，进行预训练的任务</strong></li>
<li><strong>inner layer 的参数保持不变</strong>，<strong>预训练任务的loss也保持不变</strong>，从而尽可能使 embedding generator 生成的词向量与原本 embedding layer 生成的词向量在<strong>同一个空间之中</strong></li>
<li>这部分的 <strong>loss</strong> 记作 $\mathcal{L}_p\left(s^{\prime}\right)$<br><br></li>
</ul>
<p><strong>Knowledge Distillation</strong></p>
<ul>
<li>计算 <strong>embedding generator 生成的词向量</strong>与<strong>原本 embedding layer 生成的词向量</strong>的<strong>距离</strong>，使其<strong>尽可能小</strong></li>
<li>进一步帮助 embedding generator 生成的词向量与原本 embedding layer 生成的词向量<strong>在同一个空间之中</strong><script type="math/tex; mode=display">
\mathcal{L}_d\left(s^p, s^{\prime}\right)=\frac{1}{|s|} \sum_{w \in s}\left\|h^p(w)-h^{\prime}(w)\right\|^2</script><br></li>
</ul>
<p><strong>训练embedding generator的总loss</strong></p>
<script type="math/tex; mode=display">
\mathcal{L}\left(s^p, s^{\prime}\right)=\mathcal{L}_p\left(s^{\prime}\right)+\lambda \mathcal{L}_d\left(s^p, s^{\prime}\right)</script><p>其中：</p>
<ul>
<li>$\lambda$ 是权衡两个loss占比的系数</li>
</ul>
<hr>
<p><br></p>
<h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a><strong>实验</strong>：</h3><h5 id="Domain-Adaptation"><a href="#Domain-Adaptation" class="headerlink" title="Domain Adaptation:"></a><strong>Domain Adaptation:</strong></h5><p><strong>Dataset:</strong></p>
<ul>
<li><strong>预训练</strong>语料 (<strong>out-of-domain</strong>): <strong>LDC</strong></li>
<li><strong>下游任务</strong>语料 (<strong>in-domain</strong>): 收集于<strong>UM-Corpus</strong> (domian: <strong>Thesis &amp; Laws</strong>)</li>
<li>根据以上情境可知，<strong>unseen token</strong>存在于<strong>in-domain</strong>语料中，不存在于out-of-domain语料中<br><br></li>
</ul>
<p><strong>Metrtics:</strong> </p>
<ul>
<li><strong>cased BLEU</strong>: caculated by mteval-v13a.pl.<br><br></li>
</ul>
<p><strong>Baselines:</strong><br>以下所有模型都是基于 <strong>Transformer</strong> 构建的。</p>
<ul>
<li><strong>Single-Run:</strong> 仅使用 in-domain 语料进行训练</li>
<li><strong>Pretrain-Finetune:</strong> 在 out-of-domain 语料上预训练，再在 in-domain 语料上进行finetune</li>
</ul>
<p>基于 <strong>Task-Specific Vocabulary</strong> 改进: 对预训练模型各个层进行重新训练</p>
<ul>
<li><strong>Downstream Vocab:</strong> 使用 in-domain 语料重新训练预训练模型</li>
<li><strong>Joint Vocab:</strong> 从 in &amp; out of domain 的多大语料库提取语料重新训练预训练模型</li>
<li><strong>BPE-Drop:</strong> 对预训练语料进行 subword regularization 来提高其鲁棒性</li>
</ul>
<p>基于 <strong>Embedding Generator</strong> 改进: 保留原本预训练模型，为 unseen token 训练 generator 来获得其 embedding</p>
<ul>
<li><strong>Random Init:</strong> 随机初始化 unseen token 的 embedding</li>
<li><strong>Word2Vec:</strong> 在 in-domain 语料上训练 Word2Vec 模型获取 unseen token 的 embedding</li>
<li><strong>Embedding Recon:</strong> 根据 unseen token 的 subwords 来获取其 embedding</li>
</ul>
<p>在下游任务上<strong>重新训练 Embedding 层</strong>: 仅重新训练预训练模型的 embedding 层</p>
<ul>
<li><strong>Independent Encoder:</strong> 随机初始化 embedding 层</li>
<li><strong>CBOW:</strong> 在 in-domain 语料上用 CBOW 训练得到 embedding 层<br><br></li>
</ul>
<p><strong>实验结果:</strong><br><img src="/images/note-3_images/3_outcome_1.png" alt></p>
<ul>
<li><strong>finetune</strong> 一个预训练模型的效果<strong>要差于</strong>直接<strong>在 in-domain 语料上训练</strong>的模型</li>
<li>应用 <strong>Task-Specific Vocabulary 能提升效果</strong></li>
<li><strong>Random Init</strong> 效果要<strong>优于</strong> <strong>Word2Vec</strong> 和 <strong>Embedding Recon</strong> ，这可能是是因为<strong>后两者</strong>完全基于 in-domain 语料进行训练，<strong>得到的词向量所在空间与预训练模型词向量所在空间差异过大</strong></li>
<li>本文提出的模型效果相对最好，可见使用subwords和hyperwords确实可以辅助embedding的获取</li>
<li>加入了 <strong>Knowledge Distillation</strong> 的模型效果进一步提升了，因为这进一步缩小了使用embedding layer和embedding generator产生的词向量的差异<br><br></li>
</ul>
<h5 id="Knowledge-Transferring："><a href="#Knowledge-Transferring：" class="headerlink" title="Knowledge Transferring："></a><strong>Knowledge Transferring：</strong></h5><p>分别进行了 <strong>Machine Translation</strong> 和 <strong>Question Generation</strong> 两个任务<br><strong>Dataset:</strong></p>
<ul>
<li><strong>Machine Translation</strong>: <strong>WMT14</strong></li>
<li><strong>Question Generation</strong>: <strong>SQuAD v1.1</strong> (input: answer &amp; passage; output: question)<br><br></li>
</ul>
<p><strong>Metrtics:</strong> </p>
<ul>
<li><strong>Machine Translation</strong>: <strong>BLEU</strong></li>
<li><strong>Question Generation</strong>: <strong>ROUGE-L</strong>; <strong>BLEU</strong>; <strong>METEOR</strong><br><br></li>
</ul>
<p><strong>Baselines:</strong></p>
<ul>
<li><strong>M-BERT</strong></li>
<li><strong>M-BART</strong></li>
<li><strong>Random Init</strong>: 与M-BERT模型架构相同，但是随机初始化的，即没有预训练权重<br><br></li>
</ul>
<p><strong>实验结果:</strong><br><img src="/images/note-3_images/3_outcome_2.png" alt></p>
<ul>
<li>本文提出的模型<strong>效果较好</strong>，且同时<strong>减少了参数量</strong>，<strong>提高了速度</strong>，这是因为 <strong>embedding generator 减少了词表长度</strong></li>
</ul>
<hr>
<p><br></p>
<h3 id="分析："><a href="#分析：" class="headerlink" title="分析："></a><strong>分析</strong>：</h3><h5 id="Impact-of-Subword-Granularity："><a href="#Impact-of-Subword-Granularity：" class="headerlink" title="Impact of Subword Granularity："></a><strong>Impact of Subword Granularity：</strong></h5><p><img src="/images/note-3_images/3_outcome_5.png" alt></p>
<ul>
<li>不同的<strong>分词粒度 (token granularity)</strong> 会影响模型的<strong>推理速度 (inference speed)</strong> 以及<strong>各token的暴露偏差 (exposure bias)</strong> ，上图中 exposure bias 用 <strong>Inference ECE (Expected Calibration Error)</strong> 表示</li>
<li>如图可见，适当的分词方式可以提高推理速度，也可以减轻 exposure bias<br><br></li>
</ul>
<h5 id="Impact-of-Embedding-Transfer："><a href="#Impact-of-Embedding-Transfer：" class="headerlink" title="Impact of Embedding Transfer："></a><strong>Impact of Embedding Transfer：</strong></h5><p><img src="/images/note-3_images/3_outcome_3.png" alt></p>
<ul>
<li>上图展示了 <strong>M-BERT</strong> 模型使用本文 embedding generator 方法 (+Ours)，以及不使用本文方法 (w/M-BERT) 的模型效果</li>
<li><strong>横坐标为 embedding generator 的训练步数</strong>，由于w/M-BERT没有embedding generator，因此其为一条直线</li>
<li>加入了 <strong>embedding generator</strong> 在<strong>各个训练步数下都优于 w/M-BERT 模型</strong><br><br></li>
</ul>
<h5 id="Computational-Costs："><a href="#Computational-Costs：" class="headerlink" title="Computational Costs："></a><strong>Computational Costs：</strong></h5><p><img src="/images/note-3_images/3_outcome_3.png" alt></p>
<ul>
<li>如图可见，使用本文 embedding generator 方法 (+Ours)的模型<strong>很快收敛</strong>了，说明需要的<strong>计算开销不大</strong></li>
<li>推测<strong>训练速度快</strong>得益于 <strong>subwords 组成复合词的共性</strong>以及<strong>本文模型架构简单</strong></li>
<li>相比于 finetune 的时间，<strong>训练 embedding generator 的时间微不足道</strong>且 <strong>embedding generator 训练完成后可以复用于多个下游任务</strong><br><br></li>
</ul>
<h5 id="Qualitative-Analysis："><a href="#Qualitative-Analysis：" class="headerlink" title="Qualitative Analysis："></a><strong>Qualitative Analysis：</strong></h5><p><img src="/images/note-3_images/3_outcome_6.png" alt></p>
<ul>
<li>上图为一个翻译示例</li>
<li><strong>M-BERT</strong> 模型将 token: dankbar <strong>不合适地划分开</strong>，导致翻译结果不佳</li>
<li>本文模型<strong>正确划分</strong>，<strong>翻译准确</strong></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-13T15:54:31.000Z" title="2023/2/13 23:54:31">2023-02-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-06T16:05:29.249Z" title="2023/3/7 00:05:29">2023-03-07</time></span><span class="level-item"><a class="link-muted" href="/categories/My-Note/">My Note</a></span><span class="level-item">9 minutes read (About 1318 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/13/note-1/">My-Note-1 | Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</a></h1><div class="content"><p><strong>Tags</strong>：<strong>ACL 2021</strong> ；<strong>Contrastive Learning</strong> ；<strong>MNMT</strong> ；<strong>Aligned Augmentation</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.09501.pdf">原文链接</a></p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><strong>多语言翻译模型</strong>有以下<strong>优点</strong>：</p>
<ul>
<li>更<strong>高效</strong>且更<strong>容易部署</strong></li>
<li>在不同语言之间共享参数会<strong>方便知识转移 (knowledge transfer)</strong></li>
</ul>
<p>但已有的多语言翻译模型已有以下<strong>不足</strong>：</p>
<ul>
<li><strong>效果</strong>不如对应语言的双语模型</li>
<li>大多关注于以<strong>英语为中心 (English-centric)</strong> 的翻译任务</li>
</ul>
<hr>
<p><br></p>
<h3 id="本文："><a href="#本文：" class="headerlink" title="本文："></a>本文：</h3><ul>
<li><strong>目标</strong>：(1)提升多语言翻译模型的<strong>性能</strong>；(2)<strong>不局限于以英语为中心</strong></li>
<li><strong>方法</strong>：(1) 使用<strong>对比学习</strong>，将<strong>不同语言中的同义句子拉近</strong>，不同义句子拉远；(2) 使用<strong>Aligned Augmentation (AA) 的数据增强</strong>方式来获取正负样本</li>
<li><strong>模型</strong>：<strong>mRASP2</strong></li>
<li><strong>创新点</strong>：(1) 利用<strong>对比学习</strong>将不同语言的同义词向量拉近，即拉到了同一个空间；(2) 利用了<strong>单语语料</strong> (monolingual)，并将 <strong>AA</strong> 拓展到了单语的预训练任务上</li>
</ul>
<hr>
<p><br></p>
<h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><h5 id="模型架构："><a href="#模型架构：" class="headerlink" title="模型架构："></a>模型架构：</h5><p>本文的模型基于 Transformer 构建，由12层 Encoder 和12层 Decoder 组成<br><img src="/images/note-1_images/method_1.png" alt></p>
<ul>
<li>将<strong>句子经过Encoder之后的向量</strong>进行<strong>对比学习</strong></li>
<li><strong>不同语言中相同意思的句子互为正例</strong>，其他意思的句子作为负例进行对比学习</li>
</ul>
<p><strong>翻译的Loss：</strong></p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{cc}}=\sum_{\mathbf{x}^{i}, \mathbf{x}^{j} \in \mathcal{D}}-\log P_{\theta}\left(\mathbf{x}^{i} \mid \mathbf{x}^{j}\right)</script><p>其中：</p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.113ex" height="1.879ex" role="img" focusable="false" viewbox="0 -830.4 934 830.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></g></svg></mjx-container> 为源语言 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewbox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container> 的句子，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.22ex" height="1.879ex" role="img" focusable="false" viewbox="0 -830.4 981.3 830.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></svg></mjx-container> 为目标语言 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewbox="0 -661 412 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></svg></mjx-container> 的句子</li>
</ul>
<p><strong>对比学习的Loss：</strong></p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{ctr}}=-\sum_{\mathbf{x}_{i}, \mathbf{x}_{j} \in \mathcal{D}} \log \frac{e^{\operatorname{sim}^{+}\left(\mathcal{R}\left(\mathbf{x}^{i}\right), \mathcal{R}\left(\mathbf{x}^{j}\right)\right) / \tau}}{\sum_{\mathbf{y}^{j}} e^{\operatorname{sim}^{-}\left(\mathcal{R}\left(\mathbf{x}^{i}\right), \mathcal{R}\left(\mathbf{y}^{j}\right)\right) / \tau}}</script><p>其中：</p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.113ex" height="1.879ex" role="img" focusable="false" viewbox="0 -830.4 934 830.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></g></svg></mjx-container> 和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.22ex" height="1.879ex" role="img" focusable="false" viewbox="0 -830.4 981.3 830.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></svg></mjx-container> 为语言 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewbox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container> 和语言 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewbox="0 -661 412 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></svg></mjx-container> 的同义句子</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.113ex" height="1.879ex" role="img" focusable="false" viewbox="0 -830.4 934 830.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></g></svg></mjx-container> 和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="2.22ex" height="2.331ex" role="img" focusable="false" viewbox="0 -830.4 981.3 1030.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D432" d="M84 -102Q84 -110 87 -119T102 -138T133 -149Q148 -148 162 -143T186 -131T206 -114T222 -95T234 -76T243 -59T249 -45T252 -37L269 0L96 382H26V444H34Q49 441 146 441Q252 441 270 444H279V382H255Q232 382 232 380L337 151L442 382H394V444H401Q413 441 495 441Q568 441 574 444H580V382H510L406 152Q298 -84 297 -87Q269 -139 225 -169T131 -200Q85 -200 54 -172T23 -100Q23 -64 44 -50T87 -35Q111 -35 130 -50T152 -92V-100H84V-102Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(640,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></g></svg></mjx-container> 为语言 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewbox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container> 和语言 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="0.932ex" height="1.957ex" role="img" focusable="false" viewbox="0 -661 412 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></svg></mjx-container> 的不同义句子，当然使用别的语言也是可以的</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.429ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2399.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="52" d="M37 475Q19 475 19 487Q19 503 35 530T83 589T180 647T327 682H374Q387 682 417 682T464 683Q519 683 559 679T642 663T708 625T731 557Q731 481 668 411T504 300Q506 296 512 286T528 257T553 202Q594 105 611 82Q635 47 665 47Q708 47 742 93Q758 113 786 128Q804 136 819 137Q837 137 837 125Q837 115 818 92T767 43T687 -2T589 -22Q549 -22 517 22T467 120T422 221T362 273Q346 273 346 287Q348 301 373 320T436 342Q437 342 446 343T462 345T481 348T504 353T527 362T553 375T577 393Q598 412 614 443T630 511Q630 545 613 566T541 600T393 614Q370 614 370 613L366 584Q349 446 311 307T243 96L213 25Q205 8 179 -7T132 -22Q125 -22 120 -18T117 -8Q117 -5 130 26T163 113T205 239T246 408T274 606V614Q273 614 259 613T231 609T198 602T163 588Q131 572 113 518Q102 502 80 490T37 475Z"/></g></g><g data-mml-node="mrow" transform="translate(1014.7,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g><g data-mml-node="mo" transform="translate(996,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></g></svg></mjx-container> 为句子 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewbox="0 -444 607 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"/></g></g></g></g></svg></mjx-container> 经过 Encoder 的平均池化输出</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex;" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewbox="0 -431 517 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"/></g></g></g></svg></mjx-container>  为温度系数(Temperature)，用于控制正负例的差异程度<br><br></li>
</ul>
<p><strong>模型总Loss：</strong></p>
<script type="math/tex; mode=display">
\mathcal{L}=\mathcal{L}_{\mathrm{ce}}+\lambda|s| \mathcal{L}_{\mathrm{ctr}}</script><p>其中：</p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.027ex;" xmlns="http://www.w3.org/2000/svg" width="1.319ex" height="1.597ex" role="img" focusable="false" viewbox="0 -694 583 706"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"/></g></g></g></svg></mjx-container> 是权衡两个loss占比的系数</li>
<li>由于 $\mathcal{L}<em>{\mathrm{ce}}$ 是<strong>以token为单位</strong>计算的，$\mathcal{L}</em>\mathrm{ctr}$ 是<strong>以句子为单位</strong>计算的，因此要在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="3.709ex" height="1.952ex" role="img" focusable="false" viewbox="0 -705 1639.2 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(723,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"/><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(444,0)"/><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(833,0)"/></g></g></g></g></g></g></svg></mjx-container> 上<strong>乘上句子的平均长度</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex;" xmlns="http://www.w3.org/2000/svg" width="2.319ex" height="2.26ex" role="img" focusable="false" viewbox="0 -749.5 1025 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mo" transform="translate(747,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g></g></g></svg></mjx-container> ，此部分的平均长度应该是一个样本的对比学习中所有正负例的句子长度的平均值<br><br></li>
</ul>
<h5 id="数据增强方法：Aligned-Augmentation-AA"><a href="#数据增强方法：Aligned-Augmentation-AA" class="headerlink" title="数据增强方法：Aligned Augmentation (AA)"></a><strong>数据增强方法：Aligned Augmentation (AA)</strong></h5><ul>
<li><strong>AA</strong> 是基于 <strong>Random Aligned Substitution technique (RAS)</strong> 的数据增强方法</li>
<li><p>将输入encoder的一句话中的部分 token 替换为其他语言的同义 token</p>
<p><strong>AA for Parallel Corpora：</strong><br><img src="/images/note-1_images/method_2.png" alt></p>
<p><strong>AA for Monolingual Corpora：</strong><br><img src="/images/note-1_images/method_3.png" alt></p>
</li>
</ul>
<hr>
<p><br></p>
<h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><p><strong>Dataset：</strong></p>
<ul>
<li><strong>Parallel Dataset PC32</strong>：平行语料；包含32种语言；以英语为中心</li>
<li><strong>Monolingual Dataset MC24</strong>：单语语料；包含24种语言，其中21种也包含于PC32中；使用 temperature sampling</li>
<li><strong>WMT, IWSLT, OPUS-100</strong>: 作为测试集</li>
<li>temperature sampling公式为：<script type="math/tex; mode=display">\tilde{n}_{i}=\left(n_{i} / \sum_{j} n_{j}\right)^{1 / T},   T=5</script></li>
</ul>
<p><strong>Baseline：</strong></p>
<ul>
<li><strong>Transformer</strong></li>
<li><strong>Adapter</strong>: a trade-off between unified multilingual model and bilingual model</li>
<li><strong>mBART</strong>: </li>
<li><strong>XLM</strong>: </li>
<li><strong>MASS</strong>: </li>
<li><strong>mRASP</strong>: </li>
<li><strong>MASS</strong>: </li>
<li><strong>Multi-Distillation</strong>: Adapter with selective distillation methods<br><br></li>
</ul>
<h5 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a><strong>实验结果：</strong></h5><p><strong>Supervised Directions：</strong><br><img src="/images/note-1_images/outcome_1.png" alt></p>
<p><strong>Unsupervised Directions：</strong><br><img src="/images/note-1_images/outcome_2.png" alt></p>
<p><strong>Zero-Shot：</strong><br><img src="/images/note-1_images/outcome_3.png" alt></p>
<ul>
<li>Pivot是基于中枢的方法，即虽然没有学习 A-&gt;C ，通多 A-&gt;B, B-&gt;C 进行翻译</li>
</ul>
<hr>
<p><br></p>
<h3 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h3><h5 id="Ablation-Study："><a href="#Ablation-Study：" class="headerlink" title="Ablation Study："></a><strong>Ablation Study：</strong></h5><p><img src="/images/note-1_images/outcome_4.png" alt><br>其中：</p>
<ul>
<li><strong>(2) mRASP</strong> 不含MC24和对比学习部分</li>
<li><strong>(3) mRASP2 w/o AA</strong> 不含AA和MC24</li>
<li><strong>(4) mRASP2 w/o MC24</strong> 不含MC24部分</li>
</ul>
<p><strong>(1) v.s. (3)</strong> 可知：<br>加入了<strong>对比学习</strong>后，模型<strong>zero-shot方面效果大幅提高</strong>，同时其<strong>他方面没有变差</strong></p>
<p><strong>(2) v.s. (4)</strong> 可知：<br><strong>对比学习</strong>对于<strong>zero-shot</strong>至关重要</p>
<p>由 <strong>(5)</strong> 可知：<br>加入了 CTL，AA，MC24 之后，模型的效果在各方面都明显提高了，尤其是<strong>非监督</strong>方面<br><br></p>
<h5 id="Similarity-Search："><a href="#Similarity-Search：" class="headerlink" title="Similarity Search："></a><strong>Similarity Search：</strong></h5><p>为了证明模型有将同义句子向量拉近到一起的功能，进行了Similarity Search的实验，具体为在另一种语言中找到与某句句子的向量最接近的句子，再查看这两句不同语言的句子是不是对应相同意思，最终统计意思相同的概率。</p>
<ul>
<li>以英语为中心的语料：Tatoeba dataset<br><img src="/images/note-1_images/outcome_5.png" alt></li>
<li><p>不以英语为中心的语料：Ted-M<br><img src="/images/note-1_images/outcome_6.png" alt></p>
</li>
<li><p>总体结果为：mTransformer &lt; mRASP2 w/o AA &lt; mRASP2</p>
</li>
<li>在以英语为中心的语料中，容量越小的语言 mRASP2 的提升效果越好<br><br></li>
</ul>
<h5 id="Visualization："><a href="#Visualization：" class="headerlink" title="Visualization："></a><strong>Visualization：</strong></h5><p>将 Ted-M 中的句子用Encoder得到向量，再通过 T-SNE 降为2维作可视化<br>下图仅展示了效果比较明显的英语，日语和德语<br><img src="/images/note-1_images/outcome_7.png" alt></p>
<ul>
<li>可见mRASP2中不同语言中的向量分布更加集中在了相近的空间，而不是像m-Transformer那样杂乱且交错</li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/%E5%A4%B4%E5%83%8F.png" alt="Yang Yan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Yang Yan</p><p class="is-size-6 is-block">Master of Software Engineering</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai University of Finance and Economics</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">6</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Yang-Yan-Yang-Yan" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Yang-Yan-Yang-Yan"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/My-Note/"><span class="level-start"><span class="level-item">My Note</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-06T16:00:00.000Z">2023-03-07</time></p><p class="title"><a href="/2023/03/07/note-3/">My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</a></p><p class="categories"><a href="/categories/My-Note/">My Note</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-15T14:07:37.000Z">2023-02-15</time></p><p class="title"><a href="/2023/02/15/note-2/">note-2</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-13T15:54:31.000Z">2023-02-13</time></p><p class="title"><a href="/2023/02/13/note-1/">My-Note-1 | Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</a></p><p class="categories"><a href="/categories/My-Note/">My Note</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-30T12:40:25.903Z">2022-12-30</time></p><p class="title"><a href="/2022/12/30/hello-world/">Hello World</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ACL-2021/"><span class="tag">ACL 2021</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Aligned-Augmentation/"><span class="tag">Aligned Augmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Contrastive-Learning/"><span class="tag">Contrastive Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MNMT/"><span class="tag">MNMT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLG/"><span class="tag">NLG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subword/"><span class="tag">Subword</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2023 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>