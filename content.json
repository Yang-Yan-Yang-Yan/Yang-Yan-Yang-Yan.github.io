{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/12/30/hello-world/"},{"title":"My-Note-1 | Contrastive Learning for Many-to-many Multilingual Neural Machine Translation","text":"Tags：ACL 2021 ；Contrastive Learning ；MNMT ；Aligned Augmentation 原文链接 背景多语言翻译模型有以下优点： 更高效且更容易部署 在不同语言之间共享参数会方便知识转移 (knowledge transfer) 但已有的多语言翻译模型已有以下不足： 效果不如对应语言的双语模型 大多关注于以英语为中心 (English-centric) 的翻译任务 本文： 目标：(1)提升多语言翻译模型的性能；(2)不局限于以英语为中心 方法：(1) 使用对比学习，将不同语言中的同义句子拉近，不同义句子拉远；(2) 使用Aligned Augmentation (AA) 的数据增强方式来获取正负样本 模型：mRASP2 创新点：(1) 利用对比学习将不同语言的同义词向量拉近，即拉到了同一个空间；(2) 利用了单语语料 (monolingual)，并将 AA 拓展到了单语的预训练任务上 方法：模型架构：本文的模型基于 Transformer 构建，由12层 Encoder 和12层 Decoder 组成 将句子经过Encoder之后的向量进行对比学习 不同语言中相同意思的句子互为正例，其他意思的句子作为负例进行对比学习 翻译的Loss： \\mathcal{L}_{\\mathrm{cc}}=\\sum_{\\mathbf{x}^{i}, \\mathbf{x}^{j} \\in \\mathcal{D}}-\\log P_{\\theta}\\left(\\mathbf{x}^{i} \\mid \\mathbf{x}^{j}\\right)其中： 为源语言 的句子， 为目标语言 的句子 对比学习的Loss： \\mathcal{L}_{\\mathrm{ctr}}=-\\sum_{\\mathbf{x}_{i}, \\mathbf{x}_{j} \\in \\mathcal{D}} \\log \\frac{e^{\\operatorname{sim}^{+}\\left(\\mathcal{R}\\left(\\mathbf{x}^{i}\\right), \\mathcal{R}\\left(\\mathbf{x}^{j}\\right)\\right) / \\tau}}{\\sum_{\\mathbf{y}^{j}} e^{\\operatorname{sim}^{-}\\left(\\mathcal{R}\\left(\\mathbf{x}^{i}\\right), \\mathcal{R}\\left(\\mathbf{y}^{j}\\right)\\right) / \\tau}}其中： 和 为语言 和语言 的同义句子 和 为语言 和语言 的不同义句子，当然使用别的语言也是可以的 为句子 经过 Encoder 的平均池化输出 为温度系数(Temperature)，用于控制正负例的差异程度 模型总Loss： \\mathcal{L}=\\mathcal{L}_{\\mathrm{ce}}+\\lambda|s| \\mathcal{L}_{\\mathrm{ctr}}其中： 是权衡两个loss占比的系数 由于 $\\mathcal{L}{\\mathrm{ce}}$ 是以token为单位计算的，$\\mathcal{L}\\mathrm{ctr}$ 是以句子为单位计算的，因此要在 上乘上句子的平均长度 ，此部分的平均长度应该是一个样本的对比学习中所有正负例的句子长度的平均值 数据增强方法：Aligned Augmentation (AA) AA 是基于 Random Aligned Substitution technique (RAS) 的数据增强方法 将输入encoder的一句话中的部分 token 替换为其他语言的同义 token AA for Parallel Corpora： AA for Monolingual Corpora： 实验：Dataset： Parallel Dataset PC32：平行语料；包含32种语言；以英语为中心 Monolingual Dataset MC24：单语语料；包含24种语言，其中21种也包含于PC32中；使用 temperature sampling WMT, IWSLT, OPUS-100: 作为测试集 temperature sampling公式为：\\tilde{n}_{i}=\\left(n_{i} / \\sum_{j} n_{j}\\right)^{1 / T}, T=5 Baseline： Transformer Adapter: a trade-off between unified multilingual model and bilingual model mBART: XLM: MASS: mRASP: MASS: Multi-Distillation: Adapter with selective distillation methods 实验结果：Supervised Directions： Unsupervised Directions： Zero-Shot： Pivot是基于中枢的方法，即虽然没有学习 A-&gt;C ，通多 A-&gt;B, B-&gt;C 进行翻译 分析：Ablation Study：其中： (2) mRASP 不含MC24和对比学习部分 (3) mRASP2 w/o AA 不含AA和MC24 (4) mRASP2 w/o MC24 不含MC24部分 (1) v.s. (3) 可知：加入了对比学习后，模型zero-shot方面效果大幅提高，同时其他方面没有变差 (2) v.s. (4) 可知：对比学习对于zero-shot至关重要 由 (5) 可知：加入了 CTL，AA，MC24 之后，模型的效果在各方面都明显提高了，尤其是非监督方面 Similarity Search：为了证明模型有将同义句子向量拉近到一起的功能，进行了Similarity Search的实验，具体为在另一种语言中找到与某句句子的向量最接近的句子，再查看这两句不同语言的句子是不是对应相同意思，最终统计意思相同的概率。 以英语为中心的语料：Tatoeba dataset 不以英语为中心的语料：Ted-M 总体结果为：mTransformer &lt; mRASP2 w/o AA &lt; mRASP2 在以英语为中心的语料中，容量越小的语言 mRASP2 的提升效果越好 Visualization：将 Ted-M 中的句子用Encoder得到向量，再通过 T-SNE 降为2维作可视化下图仅展示了效果比较明显的英语，日语和德语 可见mRASP2中不同语言中的向量分布更加集中在了相近的空间，而不是像m-Transformer那样杂乱且交错","link":"/2023/02/13/note-1/"},{"title":"note-2","text":"","link":"/2023/02/15/note-2/"},{"title":"My-Note-3 | Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation","text":"Tags：ACL 2021 ；NLG ；Subword 原文链接 背景：基于 pretrain-finetune 的模型在 pretrain 时使用统一的语料库 (one-size-fits-all vocabulary)，而 finetune 时的语料库则随具体任务而不同。pretrain 时的语料库和 finetune 时的语料库中的 subwords 分布往往会有所不同，这导致了: pretrain 时学到的 subwords 划分会更加细粒度从而覆盖其更大的语料库，但这会使得 finetune 时的 exposure bias 更严重且计算开销更大 在 pretrain 时不常见，但在 finetune 时常见的 token 可能会被错误划分为 subwords，导致语义保留不佳 本文： 目标：改善上下游任务中由于 subwords 分布不同导致的下游任务中部分token表示不佳 (under-represented) 方法：单独训练一个 embedding generator，输入一个 token，根据其 subwords 和 hyperwords 的词向量来得到该token的词向量，改善下游任务中 under-represented token 的词向量表达 模型：AVG-EG; ATT-EG; PATT-EG 创新点：(1) 从克服 subwords 分布差异 的角度出发，有效改善了模型; (2) 计算效率高，在简单地单独训练后可以即插即用 (plug-and-play) ; (3) 对 under-represented token 的词向量表示进行了多个方法的探索，包括平均，基于注意力机制和基于语素 (morphemes) 信息的方法。 方法：模型架构：模型基于标准的 pretrain-finetune 模式构建 在 pretrain 时，额外训练一个 generator ，其可以输出任何输入 token 的词向量 在 finetune 时，分为 embedding layer 和 inner layer 两部分，embedding layer 中首先基于下游任务的语料库学习 subwords 划分方法以及对应的词表，其中的 token 如果存在于预训练模型的词表中，则直接使用预训练的 embedding layer 得到词向量，否则使用 generator 生成其词向量；inner layer 部分延用预训练模型的参数。 构建 Embedding Generator：Subwords &amp; Hyperwords: 一个 token 的语义信息与其 subwords 和 hyperwords 有关，以 motorcycle 为例，其 subwords 包含 motor 和 cycle，反之，motor 的 hyperwords 包含 motorcycle 在下游任务词表中发现一个上游任务词表中不存在的token (unseen token) 时，获取上游词表中其所有的 subwords 和 hyperwords ，根据这些 token 的 embedding 计算 unseen token 的 embedding AVG-EG: Averaging-Based Embedding Generator 将 unseen token 的所有 subwords 和 hyperwords 的 embedding 平均作为 unseen token 的 embedding G(w)=\\frac{1}{\\left|S_{m}(w)\\right|} \\sum_{w^{\\prime} \\in S_{m}(w)} E\\left(w^{\\prime}\\right) 其中： $w$ 为下游词表的 unseen token $w^{\\prime}$ 为上游词表中与 $w$ 有关的 token (subwords or hyperwords) $S_{m}(w)$ 是 $w^{\\prime}$ 的集合 $E\\left(w^{\\prime}\\right)$ 即为 $w^{\\prime}$ 在预训练模型的 embedding 层得到的词向量 ATT-EG: Attention-Based Embedding Generator 将 $S_{m}(w)$ 中的所有 $w^{\\prime}$ 通过 attention 机制计算 $w$ 的词向量 G(w)=\\frac{1}{\\left|S_{m}(w)\\right|} \\sum_{w^{\\prime} \\in S_{m}(w)} \\alpha\\left(w^{\\prime}\\right) \\cdot E\\left(w^{\\prime}\\right) \\alpha\\left(w^{\\prime}\\right)=\\frac{\\exp \\left(\\mathbf{W}^{\\top} E\\left(w^{\\prime}\\right)\\right)}{\\sum_{w^{\\prime \\prime} \\in S_{m}(w)} \\exp \\left(\\mathbf{W}^{\\top} E\\left(w^{\\prime \\prime}\\right)\\right)}其中： $\\mathbf{W}$ 为可学习的参数 PATT-EG: Position-Aware Attention-Based Embedding Generator 为 $w$ 和 $w^{\\prime}$ 定义了六种关系 如果 $w$ 是 $w^{\\prime}$ 的 subword，那 $w$ 可能是 $w^{\\prime}$ 的 prefix / infix / suffix 如果 $w$ 是 $w^{\\prime}$ 的 hyperword，那 $w^{\\prime}$ 可能是 $w$ 的 prefix / infix / suffix 根据 $w$ 和 $w^{\\prime}$ 之间的关系确定计算公式 G(w)=\\frac{1}{\\left|S_{m}(w)\\right|} \\sum_{w^{\\prime} \\in S_{m}(w)} \\alpha\\left(w^{\\prime}\\right) \\cdot E\\left(w^{\\prime}\\right) \\alpha\\left(w^{\\prime}\\right)=\\frac{\\exp \\left(\\mathbf{I{W_r}}E\\left(w^{\\prime}\\right)\\right)}{\\sum_{w^{\\prime \\prime} \\in S_{m}(w)} \\exp \\left(\\mathbf{I{W_r}} E\\left(w^{\\prime \\prime}\\right)\\right)}其中： $\\mathbf{W_r}$ 为可学习的参数 $\\mathbf{I}$ 为 one-hot 向量，指示 $w$ 和 $w^{\\prime}$ 之间的关系 训练 Embedding Generator： 预处理上游语料库获取 unseen token 在上游语料库中随机选取连续的 token ，拼接为一个 unseen token 在上游语料库中随机选取 token ，将其拆分为多个 unseen token Reusing Pre-training Loss 根据上面获取的 unseen token，使用 embedding generator 获取其 embedding，代入预训练模型的 inner layer，进行预训练的任务 inner layer 的参数保持不变，预训练任务的loss也保持不变，从而尽可能使 embedding generator 生成的词向量与原本 embedding layer 生成的词向量在同一个空间之中 这部分的 loss 记作 $\\mathcal{L}_p\\left(s^{\\prime}\\right)$ Knowledge Distillation 计算 embedding generator 生成的词向量与原本 embedding layer 生成的词向量的距离，使其尽可能小 进一步帮助 embedding generator 生成的词向量与原本 embedding layer 生成的词向量在同一个空间之中 \\mathcal{L}_d\\left(s^p, s^{\\prime}\\right)=\\frac{1}{|s|} \\sum_{w \\in s}\\left\\|h^p(w)-h^{\\prime}(w)\\right\\|^2 训练embedding generator的总loss \\mathcal{L}\\left(s^p, s^{\\prime}\\right)=\\mathcal{L}_p\\left(s^{\\prime}\\right)+\\lambda \\mathcal{L}_d\\left(s^p, s^{\\prime}\\right)其中： $\\lambda$ 是权衡两个loss占比的系数 实验：Domain Adaptation:Dataset: 预训练语料 (out-of-domain): LDC 下游任务语料 (in-domain): 收集于UM-Corpus (domian: Thesis &amp; Laws) 根据以上情境可知，unseen token存在于in-domain语料中，不存在于out-of-domain语料中 Metrtics: cased BLEU: caculated by mteval-v13a.pl. Baselines:以下所有模型都是基于 Transformer 构建的。 Single-Run: 仅使用 in-domain 语料进行训练 Pretrain-Finetune: 在 out-of-domain 语料上预训练，再在 in-domain 语料上进行finetune 基于 Task-Specific Vocabulary 改进: 对预训练模型各个层进行重新训练 Downstream Vocab: 使用 in-domain 语料重新训练预训练模型 Joint Vocab: 从 in &amp; out of domain 的多大语料库提取语料重新训练预训练模型 BPE-Drop: 对预训练语料进行 subword regularization 来提高其鲁棒性 基于 Embedding Generator 改进: 保留原本预训练模型，为 unseen token 训练 generator 来获得其 embedding Random Init: 随机初始化 unseen token 的 embedding Word2Vec: 在 in-domain 语料上训练 Word2Vec 模型获取 unseen token 的 embedding Embedding Recon: 根据 unseen token 的 subwords 来获取其 embedding 在下游任务上重新训练 Embedding 层: 仅重新训练预训练模型的 embedding 层 Independent Encoder: 随机初始化 embedding 层 CBOW: 在 in-domain 语料上用 CBOW 训练得到 embedding 层 实验结果: finetune 一个预训练模型的效果要差于直接在 in-domain 语料上训练的模型 应用 Task-Specific Vocabulary 能提升效果 Random Init 效果要优于 Word2Vec 和 Embedding Recon ，这可能是是因为后两者完全基于 in-domain 语料进行训练，得到的词向量所在空间与预训练模型词向量所在空间差异过大 本文提出的模型效果相对最好，可见使用subwords和hyperwords确实可以辅助embedding的获取 加入了 Knowledge Distillation 的模型效果进一步提升了，因为这进一步缩小了使用embedding layer和embedding generator产生的词向量的差异 Knowledge Transferring：分别进行了 Machine Translation 和 Question Generation 两个任务Dataset: Machine Translation: WMT14 Question Generation: SQuAD v1.1 (input: answer &amp; passage; output: question) Metrtics: Machine Translation: BLEU Question Generation: ROUGE-L; BLEU; METEOR Baselines: M-BERT M-BART Random Init: 与M-BERT模型架构相同，但是随机初始化的，即没有预训练权重 实验结果: 本文提出的模型效果较好，且同时减少了参数量，提高了速度，这是因为 embedding generator 减少了词表长度 分析：Impact of Subword Granularity： 不同的分词粒度 (token granularity) 会影响模型的推理速度 (inference speed) 以及各token的暴露偏差 (exposure bias) ，上图中 exposure bias 用 Inference ECE (Expected Calibration Error) 表示 如图可见，适当的分词方式可以提高推理速度，也可以减轻 exposure bias Impact of Embedding Transfer： 上图展示了 M-BERT 模型使用本文 embedding generator 方法 (+Ours)，以及不使用本文方法 (w/M-BERT) 的模型效果 横坐标为 embedding generator 的训练步数，由于w/M-BERT没有embedding generator，因此其为一条直线 加入了 embedding generator 在各个训练步数下都优于 w/M-BERT 模型 Computational Costs： 如图可见，使用本文 embedding generator 方法 (+Ours)的模型很快收敛了，说明需要的计算开销不大 推测训练速度快得益于 subwords 组成复合词的共性以及本文模型架构简单 相比于 finetune 的时间，训练 embedding generator 的时间微不足道且 embedding generator 训练完成后可以复用于多个下游任务 Qualitative Analysis： 上图为一个翻译示例 M-BERT 模型将 token: dankbar 不合适地划分开，导致翻译结果不佳 本文模型正确划分，翻译准确","link":"/2023/03/07/note-3/"}],"tags":[{"name":"ACL 2021","slug":"ACL-2021","link":"/tags/ACL-2021/"},{"name":"Contrastive Learning","slug":"Contrastive-Learning","link":"/tags/Contrastive-Learning/"},{"name":"MNMT","slug":"MNMT","link":"/tags/MNMT/"},{"name":"Aligned Augmentation","slug":"Aligned-Augmentation","link":"/tags/Aligned-Augmentation/"},{"name":"NLG","slug":"NLG","link":"/tags/NLG/"},{"name":"Subword","slug":"Subword","link":"/tags/Subword/"}],"categories":[{"name":"My Note","slug":"My-Note","link":"/categories/My-Note/"}],"pages":[]}