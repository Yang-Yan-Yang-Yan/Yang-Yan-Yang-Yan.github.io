{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/12/30/hello-world/"},{"title":"trial","text":"","link":"/2020/04/10/trial/"},{"title":"Contrastive Learning for Many-to-many Multilingual Neural Machine Translation","text":"背景多语言翻译模型有以下优点： 更高效且更容易部署 在不同语言之间共享参数会方便知识转移 (knowledge transfer)但已有的多语言翻译模型已有以下不足： 效果不如对应语言的双语模型 大多关注于以英语为中心 (English-centric) 的翻译任务 本文： 目标：(1)提升多语言翻译模型的性能；(2)不局限于以英语为中心 方法：(1) 使用对比学习，将不同语言中的同义句子拉近，不同义句子拉远；(2) 使用Aligned Augmentation (AA) 的数据增强方式来获取正负样本 模型：mRASP2 创新点：(1) 利用对比学习将不同语言的同义词向量拉近，即拉到了同一个空间；(2) 利用了单语语料 (monolingual)，并将 AA 拓展到了单语的预训练任务上 方法：模型架构：本文的模型基于Transformer构建，由12层Encoder和12层Decoder组成![[feeed243d06047ce6dc9f31f9969ff58_0_Figure_1.png]] 将句子经过Encoder之后的向量进行对比学习 不同语言中相同意思的句子互为正例，其他意思的句子作为负例进行对比学习 翻译的Loss： \\mathcal{L}_{\\mathrm{cc}}=\\sum_{\\mathbf{x}^{i}, \\mathbf{x}^{j} \\in \\mathcal{D}}-\\log P_{\\theta}\\left(\\mathbf{x}^{i} \\mid \\mathbf{x}^{j}\\right)其中： 为源语言 的句子， 为目标语言 的句子 对比学习的Loss： \\mathcal{L}_{\\mathrm{ctr}}=-\\sum_{\\mathbf{x}_{i}, \\mathbf{x}_{j} \\in \\mathcal{D}} \\log \\frac{e^{\\operatorname{sim}^{+}\\left(\\mathcal{R}\\left(\\mathbf{x}^{i}\\right), \\mathcal{R}\\left(\\mathbf{x}^{j}\\right)\\right) / \\tau}}{\\sum_{\\mathbf{y}^{j}} e^{\\operatorname{sim}^{-}\\left(\\mathcal{R}\\left(\\mathbf{x}^{i}\\right), \\mathcal{R}\\left(\\mathbf{y}^{j}\\right)\\right) / \\tau}}其中： 和 为语言 和语言 的同义句子 和 为语言 和语言 的不同义句子，当然使用别的语言也是可以的 为句子 经过 Encoder 的平均池化输出 为温度系数(Temperature)，用于控制正负例的差异程度 模型总Loss： \\mathcal{L}=\\mathcal{L}_{\\mathrm{ce}}+\\lambda|s| \\mathcal{L}_{\\mathrm{ctr}}其中： 是权衡两个loss占比的系数 由于 $\\mathcal{L}{\\mathrm{ce}}$ 是以token为单位计算的，$\\mathcal{L}{\\mathrm{ctr}}$ 是以句子为单位计算的，因此要在 上乘上句子的平均长度 ，此部分的平均长度应该是一个样本的对比学习中所有正负例的句子长度的平均值 数据增强方法：Aligned Augmentation (AA) AA 是基于 Random Aligned Substitution technique (RAS) 的数据增强方法 将输入encoder的一句话中的部分 token 替换为其他语言的同义 token AA for Parallel Corpora：![[Pasted image 20230203185020.png]] AA for Monolingual Corpora：![[Pasted image 20230203185055.png]] 实验：Dataset： Parallel Dataset PC32：平行语料；包含32种语言；以英语为中心 Monolingual Dataset MC24：单语语料；包含24种语言，其中21种也包含于PC32中；使用 temperature sampling WMT, IWSLT, OPUS-100: 作为测试集 temperature sampling公式为：\\tilde{n}_{i}=\\left(n_{i} / \\sum_{j} n_{j}\\right)^{1 / T}, T=5 Baseline： Transformer Adapter: a trade-off between unified multilingual model and bilingual model mBART: XLM: MASS: mRASP: MASS: Multi-Distillation: Adapter with selective distillation methods 实验结果：Supervised Directions： ![[Pasted image 20230203192405.png]] Unsupervised Directions：![[Pasted image 20230203192521.png]] Zero-Shot：![[Pasted image 20230203192536.png]] Pivot是基于中枢的方法，即虽然没有学习 A-&gt;C ，通多 A-&gt;B, B-&gt;C 进行翻译 分析：Ablation Study：![[Pasted image 20230203192906.png]]其中： (2) mRASP 不含MC24和对比学习部分 (3) mRASP2 w/o AA 不含AA和MC24 (4) mRASP2 w/o MC24 不含MC24部分 (1) v.s. (3) 可知：加入了对比学习后，模型zero-shot方面效果大幅提高，同时其他方面没有变差 (2) v.s. (4) 可知：对比学习对于zero-shot至关重要 由 (5) 可知：加入了 CTL，AA，MC24 之后，模型的效果在各方面都明显提高了，尤其是非监督方面 Similarity Search：为了证明模型有将同义句子向量拉近到一起的功能，进行了Similarity Search的实验，具体为在另一种语言中找到与某句句子的向量最接近的句子，再查看这两句不同语言的句子是不是对应相同意思，最终统计意思相同的概率。 以英语为中心的语料：Tatoeba dataset![[Pasted image 20230203194544.png]] 不以英语为中心的语料：Ted-M![[Pasted image 20230203194611.png]] 总体结果为：mTransformer &lt; mRASP2 w/o AA &lt; mRASP2 在以英语为中心的语料中，容量越小的语言 mRASP2 的提升效果越好 Visualization：将 Ted-M 中的句子用Encoder得到向量，再通过 T-SNE 降为2维作可视化下图仅展示了效果比较明显的英语，日语和德语![[feeed243d06047ce6dc9f31f9969ff58_8_Figure_4.png]] 可见mRASP2中不同语言中的向量分布更加集中在了相近的空间，而不是像m-Transformer那样杂乱且交错","link":"/2023/02/13/note-1/"},{"title":"","text":"","link":"/2023/02/14/Pasted_image_20230203192405.png/"},{"title":"note-2","text":"","link":"/2023/02/15/note-2/"},{"title":"note-2","text":"","link":"/2023/02/15/note-2-1/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"ACL 2021","slug":"ACL-2021","link":"/tags/ACL-2021/"},{"name":"Contrastive Learning","slug":"Contrastive-Learning","link":"/tags/Contrastive-Learning/"},{"name":"MNMT","slug":"MNMT","link":"/tags/MNMT/"},{"name":"Aligned Augmentation","slug":"Aligned-Augmentation","link":"/tags/Aligned-Augmentation/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"My Note","slug":"My-Note","link":"/categories/My-Note/"}],"pages":[]}